## homework 5 and 6

This weeks homework ( *homework 5* ) is to *choose three topics* from the fast.ai course *that you feel the most unsure about* and research and study them - to reinforce your knowledge-  in your own time this week. If you uncover any handy resources/ tutorials/ videos on the internet, please feel free to share them on the # tools channel - so that others can learn from your findings too!

Example Topics include:

1. mathematical “logs” ( and log loss)
2. Test Time Augmentation (TTA)
3. Embeddings
4. Matrix Factorization, "latent factors” etc
5. How enumeration works in python
6. Numpy Broadcasting
7. Jacobian / Hessian Matrices
8. The Chain Rule
9. Stochastic Gradient Descent (SGD)
10. Momentum (for SGD)
11. The ADAM optimiser ( & ADAMw)
12.  *any other topics you may have had trouble with!*

In on our final session on Saturday the 7th of April, *we will be asking one team member from each team to please present their findings* on ONE topic that you researched.

Enjoy the easter break & see you all for the last  faster.ai session soon! (edited)

## additional homework
*Part 1: Theory*

To get a solid understanding of the underlying principles of how a Recurrent Neural Network works - for the final homework session (*homework 6*) we would like you to *read through Andrej Karpathy’s excellent RNN tutorial*, Andrej’s tutorial is located here:

https://karpathy.github.io/2015/05/21/rnn-effectiveness/

*Part 2: Practical*

Once you have understood the under-lying theory, we would like you to *walk through the official Pytorch RNN tutorial* by copying the cell blocks into a Jupyter notebook and understanding each step as you go. The first Pytorch RNN tutorial can be located here:

http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html

*Part 3: Generation!*

Once you have completed the above practical example, we would like you to complete the final section of the homework by following final RNN Pytorch tutorial *to do text generation!!!* The final tutorial can be located here:

http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html

Instead of generating names we would like you to input some other corpus of text input & see what awesome generated outputs you can come up with,  try inputing some of the following to see what RNNs are capable of:

-cooking recipes
-Holy book text
-raw code
-wikipedia
-or anything else you can think of!

*We will be asking one team at random to present* on the above task, so GL & HF & see you at the last study group session this Saturday the 7th of April! (edited)
karpathy.github.io
The Unreasonable Effectiveness of Recurrent Neural Networks
Musings of a Computer Scientist.

